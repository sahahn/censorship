{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "opponent-purchase",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import requests\n",
    "import textract\n",
    "import openpyxl\n",
    "import urllib\n",
    "import os\n",
    "from api import API_KEY\n",
    "\n",
    "def try_download(link):\n",
    "    \n",
    "    # Some possible extensions\n",
    "    if link.endswith('.html') or link.endswith('.htm'):\n",
    "        ext = 'html'\n",
    "    elif '.pdf' in link or '=pdf' in link:\n",
    "        ext = 'pdf'\n",
    "    elif '.aspx' in link:\n",
    "        ext = 'aspx'\n",
    "    elif '.docx' in link:\n",
    "        ext = 'docx'\n",
    "        \n",
    "    # Default to html?\n",
    "    else:\n",
    "        ext = 'html'\n",
    "      \n",
    "    # Downloads locally under dl_name\n",
    "    dl_name = f'temp.{ext}'\n",
    "    os.system(f'wget -O {dl_name} {link} --no-check-certificate')\n",
    "    \n",
    "    return dl_name\n",
    "\n",
    "def load_links(sheet=1):\n",
    "    \n",
    "    # Need to load with openpyxl to retain hyper-links\n",
    "    wb = openpyxl.load_workbook('live.xlsx')\n",
    "    \n",
    "    # Get's all sheets\n",
    "    sheets = wb.sheetnames\n",
    "    \n",
    "    # Select the passed sheet by int index\n",
    "    ws = wb[sheets[sheet]]\n",
    "    \n",
    "    # Populate a dictionary of links\n",
    "    # where dict key corresponds to int index\n",
    "    # in the table\n",
    "    links = {}\n",
    "    for i in range(1, ws.max_row+1):\n",
    "        \n",
    "        try:\n",
    "            link = ws.cell(row=i, column=2).hyperlink.target\n",
    "            links[i-2] = link\n",
    "            \n",
    "        # Skip any without links\n",
    "        except AttributeError:\n",
    "            pass\n",
    "\n",
    "    return links\n",
    "\n",
    "def url_to_bill_id(url):\n",
    "    '''In some cases need to go from base bill url to find bill id'''\n",
    "    \n",
    "    # Extract state name from url\n",
    "    sp = url.split('/')\n",
    "    ind = sp.index('legiscan.com')\n",
    "    state = sp[ind+1]\n",
    "    \n",
    "    # Grab masterlist from api for this state\n",
    "    response = requests.get(f'https://api.legiscan.com/?key={API_KEY}&op=getMasterList&state={state}')\n",
    "    master_list = response.json()['masterlist']\n",
    "    \n",
    "    # Generate mapping from url to bill id\n",
    "    url_mapping = {}\n",
    "    for i in master_list:\n",
    "        entry = master_list[i]\n",
    "        if 'url' in entry:\n",
    "            \n",
    "            # Lowercase url to bill id\n",
    "            url_mapping[entry['url'].lower()] = entry['bill_id']\n",
    "     \n",
    "    # Then using the url mapping, can find the bill id needed for download\n",
    "    return url_mapping[url.replace('text', 'bill')]\n",
    "\n",
    "def load_legiscan_bill_text(bill_id):\n",
    "    \n",
    "    # Use legiscan api to get bill text\n",
    "    response = requests.get(f\"https://api.legiscan.com/?key={API_KEY}&op=getBillText&id={bill_id}\")\n",
    "\n",
    "    # Decode\n",
    "    decoded = base64.b64decode(response.json()['text']['doc'])\n",
    "    \n",
    "    # Figure out if returned as html or pdf\n",
    "    resp_type = response.json()['text']['mime']\n",
    "    if 'html' in resp_type:\n",
    "        ext = 'html'\n",
    "    elif 'pdf' in resp_type:\n",
    "        ext = 'pdf'\n",
    "    else:\n",
    "        raise RuntimeError('Unknown ext:', resp_type)\n",
    "        \n",
    "    # Save as correct type\n",
    "    with open(f'temp.{ext}', 'wb') as f:\n",
    "        f.write(decoded)\n",
    "        \n",
    "    # Load the text w/ textract library\n",
    "    text = textract.process(f'temp.{ext}')\n",
    "    \n",
    "    # Remove temp file when done\n",
    "    os.remove(f'temp.{ext}') \n",
    "    \n",
    "    return text\n",
    "\n",
    "def load_site_text(url):\n",
    "    \n",
    "    # Try downloading the url directly\n",
    "    file = try_download(url)\n",
    "\n",
    "    # If did not download, return None\n",
    "    if not os.path.exists(file):\n",
    "        return None\n",
    "    \n",
    "    # Try to extract the text from the file\n",
    "    try:\n",
    "        text = textract.process(file)\n",
    "    except:\n",
    "        text = None\n",
    "\n",
    "    # Remove temp file when done\n",
    "    os.remove(file) \n",
    "    return text\n",
    "\n",
    "def process_link(link):\n",
    "    \n",
    "    # Lower case links\n",
    "    original_link = link\n",
    "    link = link.lower()\n",
    "    \n",
    "    # CASE 1 is if the bill is hosted on legiscan\n",
    "    # In this case we will try to use their api to get the file\n",
    "    if 'legiscan' in link:\n",
    "        \n",
    "        # Some links already contain the bill ID in the url\n",
    "        sp = link.split('/')\n",
    "        if 'id' in sp:\n",
    "            bill_id = sp[sp.index('id')+1]\n",
    "            return load_legiscan_bill_text(bill_id)\n",
    "            \n",
    "        # Others, we need to manually\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    # Otherwise, try just base download\n",
    "    # then extract\n",
    "    text = load_site_text(link)\n",
    "    \n",
    "    # Try with non lower case also if None\n",
    "    if text is None:\n",
    "        text = load_site_text(original_link)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "binary-warrant",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure have reference xlsx file, but avoid re-downloading\n",
    "if not os.path.exists('live.xlsx'):\n",
    "    os.system('wget https://raw.githubusercontent.com/sahahn/gag_orders/master/data/live.xlsx')\n",
    "    \n",
    "# Init directory to save bill text\n",
    "os.makedirs('bill_text', exist_ok=True)\n",
    "\n",
    "# Only need to use sheet 1, seems to contain everything\n",
    "links = load_links(sheet=1)\n",
    "\n",
    "# Iterate through each link and download if not already\n",
    "for i in links:\n",
    "    \n",
    "    # Skip already saved\n",
    "    save_loc = os.path.join('bill_text', f'{i}.txt')\n",
    "    if os.path.exists(save_loc):\n",
    "        continue\n",
    "        \n",
    "    link = links[i]\n",
    "    \n",
    "    # Try to load bill text\n",
    "    text = process_link(link)\n",
    "    \n",
    "    # If found, save\n",
    "    if text is not None:\n",
    "        \n",
    "        # Save just as text\n",
    "        with open(save_loc, 'w') as f:\n",
    "            f.write(text.decode(\"utf-8\"))\n",
    "        print('Saved:', link, i, len(text))\n",
    "    \n",
    "    # Otherwise, indicate that there was an error\n",
    "    # scraping this file\n",
    "    else:\n",
    "        print('Error with', link, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "declared-homework",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit ('bpt': conda)",
   "language": "python",
   "name": "python391jvsc74a57bd0816e2859f723fb77ad3214da0fbda681e8d4db93bd8b118618b521c0b1f5f48f"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
